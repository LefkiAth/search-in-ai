{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62131e63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import re\n",
    "from generate import generate\n",
    "# !pip install -U transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256dd3d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf8d99",
   "metadata": {},
   "source": [
    "##### $Model$ $Access$\n",
    "\n",
    "In order to access to the model we need to:\n",
    "1. Visit the github of the model: https://github.com/ML-GSAI/LLaDA?tab=readme-ov-file\n",
    "2. Clone it\n",
    "3. Create inside the folder the notebook and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name \n",
    "model_name = 'GSAI-ML/LLaDA-8B-Base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8d3e5",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece7531",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78101374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5fc123aacf46ed93962912131d89ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15569041e11941ce83cb8a2dbe394884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbd4db0cfe2446fb4a225e296736535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/766 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for GSAI-ML/LLaDA-8B-Base loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer. The library will handle the chat template automatically.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "print(f\"Tokenizer for {model_name} loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8ceb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04871b33e4374cd08b4daf7838f1b7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73e5387dd3a462186e83d76a7a81df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_llada.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n",
      "- configuration_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0d1a52645947118950b8aef924eebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_llada.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n",
      "- modeling_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17001c560a4443fe82dfe6314539f605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03912b68d3c54db0b2ed4409fc3e15aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13017bf262724bc888201dc58e216dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dcc277d9154810b1357a729ff1084a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b512d207bb4410c9825c406f6ad5d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c94e42fec1941519f99ee39822a6032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163f22cc8a3549ffb3430920a972d5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8023e35de2c467ea31222e226e8fa5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb696ed12116445eb407e95047763dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873087319a63454a8488040fadcd4431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # or torch.bfloat16 if supported\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n"
     ]
    }
   ],
   "source": [
    "print(chat_template := tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002904ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  $Zero$ $Shots$ $vs.$ $Few$ $Shots$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1347ac",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb1f66",
   "metadata": {},
   "source": [
    "For each question, it constructs a prompt instructing the model to break down the complex question into smaller, step-by-step sub-questions. It then uses the tokenizer to convert these messages into input IDs, adds an attention mask, and generates a response from the model. Finally, it decodes the model's output and stores the original question and its zero-shot decomposition. The function returns a list of dictionaries containing these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653fcfe",
   "metadata": {},
   "source": [
    "*  **`model=model`**: The actual LLaDA model we are generating text from.\n",
    "\n",
    "\n",
    "* **`prompt=input_ids`**: This is the tokenized prompt. Same idea as input_ids in Hugging Face — it's the numerical version of the input string, shaped like (1, length).\n",
    "\n",
    "\n",
    "* **`steps=128`**: This is the number of refinement steps the LLaDA model takes. LLaDA performs **masked iterative decoding**, not left-to-right greedy generation. In each step, it masks and refines parts of the output until it's confident.\n",
    "   * Higher steps → better but slower.\n",
    "\n",
    "\n",
    "* **`gen_length=128`**: The number of tokens to generate, similar to `max_new_tokens` in Hugging Face. This controls the length of the generated response, after the prompt.\n",
    "\n",
    "\n",
    "* **`block_length=32`**: LLaDA generates in blocks, not token-by-token. This means it will attempt to produce 32 new tokens in a \"block\" before deciding what to mask and refine. Smaller blocks = more fine-grained generation, larger blocks = faster but potentially rougher outputs.\n",
    "\n",
    "\n",
    "* **`temperature=0.0`** : Controls randomness.\n",
    "   * 0.0 = **fully deterministic**, always pick the most likely next token. Since the task is **analytical**, this is perfect — no creativity needed, just accuracy.\n",
    "\n",
    "\n",
    "*  **`cfg_scale=0.0`**: Classifier-Free Guidance Scale. Used in diffusion and masked decoding models. Controls how strongly the model adheres to conditioning (like the prompt).\n",
    "    * 0.0 means \"no extra guidance\".\n",
    "\n",
    "\n",
    "*  **`remasking='low_confidence'`**:  Tells the model where to mask again during the iterative decoding.\n",
    "   * \"low_confidence\" means: in each step, the model looks at what it’s least confident about, and re-generates those tokens. This is key to LLaDA’s power — instead of continuing generation linearly, it fixes its own weak spots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bdf1d",
   "metadata": {},
   "source": [
    "*For the prompt, we use the prompt structure the model has been trained to.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot_experiment(model, tokenizer, questions):\n",
    "    results = []\n",
    "\n",
    "    for item in questions:\n",
    "        print(f\"Processing (Zero-Shot) ID: {item['id']}\")\n",
    "\n",
    "        # Manually construct the prompt using your template format\n",
    "        system_prompt = (\n",
    "            \"You are a helpful assistant. Your task is to break down the following question \"\n",
    "            \"into a few smaller questions that contribute to solving the overall problem.\\n\\n\"\n",
    "            f\"Complex Question: {item['question']}\\n\\n\"\n",
    "            \"Step-by-Step plan:\"\n",
    "        )\n",
    "\n",
    "        # Manually format chat messages using the template provided\n",
    "        bos_token = tokenizer.bos_token or \"<|begin_of_text|>\"\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        start_header = \"<|start_header_id|>\"\n",
    "        end_header = \"<|end_header_id|>\"\n",
    "\n",
    "        prompt_text = (\n",
    "            f\"{bos_token}\"\n",
    "            f\"{start_header}user{end_header}\\n\\n{system_prompt.strip()}{eot_token}\"\n",
    "            f\"{start_header}assistant{end_header}\\n\\n\"\n",
    "        )\n",
    "\n",
    "        # Tokenize and convert to tensor\n",
    "        input_ids = tokenizer(prompt_text, return_tensors=\"pt\")['input_ids'].to(model.device)\n",
    "\n",
    "        # Run generation (assuming a LLaDA-specific generate function)\n",
    "        output_ids = generate(\n",
    "            model=model,\n",
    "            prompt=input_ids,\n",
    "            steps=128,\n",
    "            gen_length=128,\n",
    "            block_length=32,\n",
    "            temperature=0.0,\n",
    "            cfg_scale=0.0,\n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "\n",
    "        # Decode only the newly generated tokens\n",
    "        generated_text = tokenizer.batch_decode(\n",
    "            output_ids[:, input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item.get('decomposition', ''),\n",
    "            \"zero_shot_decomposition\": generated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db993dae",
   "metadata": {},
   "source": [
    "##### $Few$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7901bf",
   "metadata": {},
   "source": [
    "This function, performs few-shots learning.\n",
    "\n",
    "It takes a list of questions, a list of high-quality example question/decomposition pairs (`shot_examples`), and the number of examples (`num_shots`) to use.\n",
    "\n",
    "The function first uses a prompt that includes the specified number of examples, showing the model how to decompose complex questions into simpler sub-questions. Then, for each question, it sends the prompt to the language model, generates a decomposition, and stores the results. This approach helps the model understand the desired output format and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot_experiment(model, tokenizer, data, shot_examples, num_shots=3):\n",
    "    if num_shots > len(shot_examples):\n",
    "        raise ValueError(f\"You asked for {num_shots} shots, but only {len(shot_examples)} are available.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Special tokens\n",
    "    bos_token = tokenizer.bos_token or \"<|begin_of_text|>\"\n",
    "    eot_token = \"<|eot_id|>\"\n",
    "    start_header = \"<|start_header_id|>\"\n",
    "    end_header = \"<|end_header_id|>\"\n",
    "\n",
    "    for item in data:\n",
    "        print(f\"Processing ({num_shots}-Shot) ID: {item['id']}\")\n",
    "\n",
    "        # Begin with system message\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert assistant. Your task is to break down the following question into a few smaller questions that contribute to solving the overall problem. Give only the decomposition steps, not the final answer.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Add few-shot examples\n",
    "        for example in shot_examples[:num_shots]:\n",
    "            decomposition_text = example['decomposition']\n",
    "            if isinstance(decomposition_text, list):\n",
    "                decomposition_text = \"\\n\".join(decomposition_text)\n",
    "\n",
    "            messages.append({\"role\": \"user\", \"content\": example['question']})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": decomposition_text})\n",
    "\n",
    "        # Add the new user question to decompose\n",
    "        messages.append({\"role\": \"user\", \"content\": item['question']})\n",
    "\n",
    "        # Manually build the prompt\n",
    "        prompt_parts = []\n",
    "        for idx, message in enumerate(messages):\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"].strip()\n",
    "            segment = f\"{start_header}{role}{end_header}\\n\\n{content}{eot_token}\"\n",
    "            if idx == 0:\n",
    "                segment = bos_token + segment  # Only add BOS once\n",
    "            prompt_parts.append(segment)\n",
    "\n",
    "        # Add assistant header for generation\n",
    "        prompt_parts.append(f\"{start_header}assistant{end_header}\\n\\n\")\n",
    "        prompt_text = \"\".join(prompt_parts)\n",
    "\n",
    "        # Tokenize\n",
    "        input_ids = tokenizer(prompt_text, return_tensors=\"pt\")['input_ids'].to(model.device)\n",
    "\n",
    "        # Generate output using LLaDA\n",
    "        output_ids = generate(\n",
    "            model=model,\n",
    "            prompt=input_ids,\n",
    "            steps=128,\n",
    "            gen_length=128,\n",
    "            block_length=32,\n",
    "            temperature=0.0,\n",
    "            cfg_scale=0.0,\n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "\n",
    "        # Decode generated part\n",
    "        generated_text = tokenizer.batch_decode(\n",
    "            output_ids[:, input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item.get('decomposition', ''),\n",
    "            f\"{num_shots}_shot_decomposition\": generated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249d5d5",
   "metadata": {},
   "source": [
    "##### $Save$ $the$ $results$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c3cbc",
   "metadata": {},
   "source": [
    "*Once the model processes the questions, its predictions for the zero-shot and few-shot experiments are saved in the corresponding folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, folder, filename):\n",
    "\n",
    "    # Make sure the folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Construct the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "\n",
    "    # Save the file\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755aeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"LLaDA-8B-Base_Hotpot_results.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5f727",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "base_results_folder = '/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/llm_predictions/'\n",
    "zero_shot_folder = os.path.join(base_results_folder, 'zero_shot') # Zero-shot predictions\n",
    "few_shots_folder = os.path.join(base_results_folder, 'few_shot')  # Few-shot predictions\n",
    "os.makedirs(zero_shot_folder, exist_ok=True)\n",
    "os.makedirs(few_shots_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846e83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "hotpot_dataset_path = \"/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/HotpotQA_dataset/hotpot_dataset.json\" # Define the path to the hotpot dataset\n",
    "\n",
    "# Load the dataset\n",
    "with open(hotpot_dataset_path, \"r\") as file:\n",
    "    hotpot_data = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636aeeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question details:\n",
      "ID: 5a8b57f25542995d1e6f1371\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Answer: yes\n",
      "Supporting sentences: ['Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.', 'Edward Davis Wood Jr. (October 10, 1924 – December 10, 1978) was an American filmmaker, actor, writer, producer, and director.', 'Aggregating the above we conclude that the answer is: yes']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Decomposition: ['What nationality Scott Derrickson had?', 'What nationality Ed Wood had?', 'Was the nationality the same?']\n"
     ]
    }
   ],
   "source": [
    "# Display the first question\n",
    "print(\"First question details:\")\n",
    "print(\"ID:\", hotpot_data[0][\"id\"])\n",
    "print(\"Question:\", hotpot_data[0][\"question\"])\n",
    "print(\"Answer:\", hotpot_data[0][\"answer\"])\n",
    "print(\"Supporting sentences:\", hotpot_data[0][\"supporting_sentences\"])\n",
    "print(\"-\"*150)\n",
    "print(\"Decomposition:\", hotpot_data[0][\"decomposition\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "few_shot_hotpot_examples_path = \"/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/HotpotQA_dataset/hotpot_few_shot.json\"\n",
    "with open(few_shot_hotpot_examples_path, \"r\") as file:\n",
    "    shot_examples = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff0f59",
   "metadata": {},
   "source": [
    "##### $Experiments$ $Run$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (Zero-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (Zero-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (Zero-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (Zero-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (Zero-Shot) ID: 5a8e3ea95542995a26add48d\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "zero_shot_results = run_zero_shot_experiment(model, tokenizer, hotpot_data)\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731169df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (3-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (3-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (3-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (3-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "three_shot_results = run_few_shot_experiment(model, tokenizer, hotpot_data, shot_examples, num_shots=3)\n",
    "save_results_to_json(three_shot_results, few_shots_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910732be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $QDMR$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1427c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "base_results_folder = '../QDMR/llm_predictions/'\n",
    "zero_shot_folder = os.path.join(base_results_folder, 'zero_shot') # Zero-shot predictions\n",
    "few_shots_folder = os.path.join(base_results_folder, 'few_shot')  # Few-shot predictions\n",
    "os.makedirs(zero_shot_folder, exist_ok=True)\n",
    "os.makedirs(few_shots_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d3454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "qdmr_dataset_path = \"../QDMR/QDMR_dataset/qdmr_dataset.json\" # Define the path to the qdmr dataset\n",
    "\n",
    "# Load the dataset\n",
    "with open(qdmr_dataset_path, \"r\") as file:\n",
    "    qdmr_data = json.load(file)\n",
    "    print(f\"Loaded {len(qdmr_data)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a8b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question details:\n",
      "ID: CWQ_dev_WebQTest-1011_c0be4f76a5397ba6d0d06f53905e504b\n",
      "Question: What Tibetan speaking countries have a population of less than 993885000?\n",
      "Lexicon tokens: ['higher than', 'same as', 'what ', 'and ', 'than ', 'at most', 'distinct', 'two', 'at least', 'or ', 'date', 'on ', '@@14@@', 'countries', 'equal', 'hundred', 'those', 'sorted by', 'elevation', 'which ', '@@6@@', '993885000', 'was ', 'did ', 'population', 'height', 'one', 'that ', 'on', 'did', 'who', 'true', '@@2@@', '100', 'false', 'and', 'was', 'speaking', 'populations', 'who ', 'a ', 'the', 'number of ', '@@16@@', 'if ', 'where', '@@18@@', 'how', 'larger than', 'is ', 'from ', 'a', 'less', 'for each', 'are ', '@@19@@', '@@4@@', '@@11@@', 'distinct ', 'to', 'not ', 'objects', 'with ', ', ', 'lowest', 'in', 'has ', 'zero', 'in ', 'there ', 'lower than', 'highest', '@@9@@', 'than', 'size', 'multiplication', 'with', 'besides ', ',', '@@1@@', 'what', 'have', 'those ', 'of', '@@3@@', 'that', 'there', '@@10@@', '@@5@@', 'both ', '@@15@@', 'number of', 'price', 'any', 'which', 'Tibetan', 'to ', 'how ', 'when ', 'of ', 'division', 'country', 'is', 'sum', 'or', 'if', 'more', '@@12@@', 'smaller than', 'flights', '@@7@@', '@@17@@', 'for each ', 'What', 'speak', 'from', '@@13@@', 'has', 'difference', 'when', 'are', 'any ', '@@8@@', 'both', 'the ', ',  ', 'besides', 'have ', 'where ', 'not']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Decomposition: return Tibetan speaking countries\n",
      "return #1 that have a population of less than 993885000\n"
     ]
    }
   ],
   "source": [
    "# Display the first question\n",
    "print(\"First question details:\")\n",
    "print(\"ID:\", qdmr_data[0][\"id\"])\n",
    "print(\"Question:\", qdmr_data[0][\"question\"])\n",
    "print(\"Lexicon tokens:\", qdmr_data[0][\"lexicon tokens\"])\n",
    "print(\"-\"*150)\n",
    "print(\"Decomposition:\", qdmr_data[0][\"decomposition\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "few_shot_qdmr_examples_path = \"../QDMR/QDMR_dataset/qdmr_few_shot.json\"\n",
    "with open(few_shot_qdmr_examples_path, \"r\") as file:\n",
    "    shot_examples = json.load(file)\n",
    "    print(f\"Loaded {len(qdmr_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3b426",
   "metadata": {},
   "source": [
    "##### $Experiments$ $Run$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb6566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1011_c0be4f76a5397ba6d0d06f53905e504b\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1011_edc922a0faa1e47614eb7e6effe2d1a1\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1036_0b5333d98ef87008aa02d1fbc1554b05\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1036_4e73509d14bda62590480b655eee8751\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1081_1ecabf57357cb4abd089a4af52154854\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "zero_shot_results = run_zero_shot_experiment(model, tokenizer, qdmr_data)\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41faad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1011_c0be4f76a5397ba6d0d06f53905e504b\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1011_edc922a0faa1e47614eb7e6effe2d1a1\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1036_0b5333d98ef87008aa02d1fbc1554b05\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1036_4e73509d14bda62590480b655eee8751\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1081_1ecabf57357cb4abd089a4af52154854\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "three_shot_results = run_few_shot_experiment(model, tokenizer, qdmr_data, shot_examples, num_shots=3)\n",
    "save_results_to_json(three_shot_results, few_shots_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
