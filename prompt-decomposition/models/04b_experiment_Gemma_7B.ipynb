{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac0389f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd21db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "from generate import generate\n",
    "import re\n",
    "from shots_type import run_experiment\n",
    "from manage_folders import save_results_to_json, dataset_folders\n",
    "from retriever import DynamicRetriever\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "# !pip install -U transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3cad91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541ce3f",
   "metadata": {},
   "source": [
    "##### $Model$ $Access$\n",
    "\n",
    "In order to access to the model we need to:\n",
    "1. Visit the webpage of the model: https://huggingface.co/google/gemma-7b-it\n",
    "2. Log in to our HF account\n",
    "3. Click on `Terms` \n",
    "4. Write yout contact information (email and name)\n",
    "5. Accept the terms\n",
    "6. Run `from huggingface_hub import notebook_login` and `notebook_login()` \n",
    "7. Create a new token with `read` rights\n",
    "8. Copy the token and paste it in the notebook and press `Enter`\n",
    "\n",
    "\n",
    "\n",
    "*It might take some minutes*...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d90e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name\n",
    "model_name = \"google/gemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bfe71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d273d12c2d6e47b28dfcab53d41183af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae37ae",
   "metadata": {},
   "source": [
    "##### $Bnb$ $Configuration$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78f15b",
   "metadata": {},
   "source": [
    "\n",
    "The `bnb_config` creates a configuration to shrink the language model. \n",
    "\n",
    "* *Compresses the Model:* It tells the system to load the model in a \"compressed\" 4-bit format instead of its full 16-bit size.\n",
    "* *Saves Memory:* This makes the model about 4 times smaller, allowing it to run on computers with less memory (RAM and VRAM).\n",
    "* *Maintains Performance:* It uses clever tricks (like doing the actual math in 16-bit) to ensure the shrunken model is still fast and accurate.\n",
    "\n",
    "Essentially, it's a set of instructions to make a huge model fit on the computer with minimal loss in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4c48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e410cd",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff856b3",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer, makes sure there's a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23fead6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37f882cead94273ac976f96b8523a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d01bace57f4ebd95d3bafb8e4a7f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71dba5f27c74bc8ad9e0e1fd86b3517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d86176ae2f54793a60fb9766e9a711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for google/gemma-7b-it loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer. The library will handle the chat template automatically.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"pad_token not set. Setting it to eos_token for this model.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer for {model_name} loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6535325b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f19626a765a48929757089c7c36bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37eedb673df9444495cce4985cfcd0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef6ca8ecc3b49d7b5d9592d0d8f54d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697e662f57a44976a880ba7a57f3de2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73c6c9a45e4440e99c14af5017c262b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d84023c33244620b047f820b4cb88fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d785751e7b8e47b687fe8432098b31c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cefaeecad1c46f4b395b8bfb885aa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33c2db6293745baa22604b6145ff444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4b2b7",
   "metadata": {},
   "source": [
    "##### $Parameters$ $for$ $the$ $model$ $configuration:$ \n",
    "\n",
    "* **`max_new_tokens`**: This sets the maximum length of the generated response. max_new_tokens=512 tells the model, \"Do not write more than 512 new tokens after the prompt.\" This prevents it from writing forever.\n",
    "\n",
    "* **`do_sample`**:\n",
    "\t* **do_sample=False**: This forces the model to be deterministic. Every time it generates a new word, it chooses the single word that it calculates as being the most statistically likely to come next. When we compare the models, we want to compare their \"best, most probable\" attempt at the problem.\n",
    "\t* **do_sample=True**: This tells the model to be creative and less predictable. Instead of always picking the #1 most likely word, it might pick the #2 or #3 most likely word, based on a random sample (controlled by parameters like temperature).\n",
    "\n",
    "\tSince the task is analytical (decomposing a problem) and not creative, the deterministic approach is better. We choose do_sample=False for all the baseline experiments to ensure your results are stable and repeatable.\n",
    "\n",
    "* **`repetition_penalty`**:\n",
    "   * used to discourage a language model from repeating the same words or phrases during text generation.\n",
    "   * Default value: 1.0 (no penalty)\n",
    "     * Typical range: 1.1 to 2.0\n",
    "\t * 1.2 = light penalty\n",
    "\t * 1.5+ = strong penalty\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e82d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model specific configuration\n",
    "model_config = {\n",
    "    \"model_id\": model_name,\n",
    "    \"uses_system_prompt\": False,  # Gemma doesnt support a system prompt\n",
    "    # \"system_prompt\": \"You are an expert system that decomposes complex user questions into a numbered list of simple, sequential sub-questions. Each sub-question should be a direct, answerable query that contributes to a logical plan. Your output should ONLY be the sub-questions. Do not provide any explanations or other answers.\",\n",
    "    \"generation_params\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": False,\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0023902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"Gemma-7b-it_results.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147e4d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $QDMR$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3febdfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 evaluation questions.\n",
      "Loaded 5 shot examples.\n"
     ]
    }
   ],
   "source": [
    "qdmr_base_folder = '../QDMR/llm_predictions/static'\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the qdmr dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data = data_assets[\"data\"]\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797617da",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4b46b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Results saved successfully to '../QDMR/llm_predictions/static/zero_shot/Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0\n",
    ")\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a9489",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bbe80fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Results saved successfully to '../QDMR/llm_predictions/static/few_shot/3shot_Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bf385",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8deb3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 evaluation questions.\n",
      "Loaded 5 shot examples.\n"
     ]
    }
   ],
   "source": [
    "qdmr_base_folder = '../QDMR/llm_predictions/random'\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the qdmr dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data = data_assets[\"data\"]\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee64f19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Random-3-Shot Experiment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved successfully to '../QDMR/llm_predictions/random/few_shot/3shot_Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97955af2",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc87b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 evaluation questions.\n",
      "Loaded 5 shot examples.\n"
     ]
    }
   ],
   "source": [
    "qdmr_base_folder = '../QDMR/llm_predictions/dynamic'\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the qdmr dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data = data_assets[\"data\"]\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41a791d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Dynamic-3-Shot Experiment\n",
      "Initializing DynamicRetriever...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c744f47c012465ea24cb6c3c2bb2e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ba1bc42d144919bea6ae59ae37fd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ada59007bee4b5a88c2370098fbe632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56aaa718827b437c8ca5f8df684458f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9249ce7b5324490590aa407d1fa08e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1968da32064ecd9c23ca34736899c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa5fbfc28824473b35c2686e124e54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d15c4502c54a2794b2b4147282d3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78bb6881c66428d924c78a8bbc5240f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dca26dc785e4addbaf18c5462b28ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a159be74f3d541eb93dc1efcd6c72300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever initialized and example embeddings are pre-computed.\n",
      "Results saved successfully to '../QDMR/llm_predictions/dynamic/few_shot/3shot_Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c6aa7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebdacf7",
   "metadata": {},
   "source": [
    "We use a helper function `dataset_folders(base_results_folder, dataset_path, few_shot_examples_path)` that streamlines the setup process for running experiments on a new dataset. It handles three key tasks:\n",
    "\n",
    "1.  **Creates Output Directories:** It takes a base folder path and automatically creates the `zero_shot` and `few_shot` subdirectories where the model's predictions will be saved.\n",
    "2.  **Loads Evaluation Data:** It reads the main dataset file (e.g., `hotpot_dataset.json`) containing the questions to be evaluated.\n",
    "3.  **Loads Few-Shot Examples:** It reads the corresponding file containing the high-quality examples for few-shot prompting.\n",
    "\n",
    "The function returns a single, convenient dictionary containing all these assets (the loaded data and the output folder paths), which can then be easily used by the main experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aee5c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 evaluation questions.\n",
      "Loaded 5 shot examples.\n"
     ]
    }
   ],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_dataset/hotpot_dataset.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_dataset/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data = data_assets[\"data\"]\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93977ab",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ec3305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (0-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (0-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (0-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (0-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (0-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results saved successfully to '../HotpotQA/llm_predictions/zero_shot/Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3441f",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec682510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (3-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (3-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (3-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (3-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results saved successfully to '../HotpotQA/llm_predictions/few_shot/3shot_Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095e598",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $StrategyQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d5f71f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 evaluation questions.\n",
      "Loaded 5 shot examples.\n"
     ]
    }
   ],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_dataset/strategyqa_dataset.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_dataset/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data = data_assets[\"data\"]\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ff347",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb037fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (0-Shot) ID: 7a0e419ffb6009156828\n",
      "Processing (0-Shot) ID: 427fe3968e32005479b9\n",
      "Processing (0-Shot) ID: 06b9ed3f803e3d5796ed\n",
      "Processing (0-Shot) ID: 5090f573b09ac3050824\n",
      "Processing (0-Shot) ID: d912709b7341dd86ba39\n",
      "Results saved successfully to '../StrategyQA/llm_predictions/zero_shot/Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c1355a",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48cd9d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: 7a0e419ffb6009156828\n",
      "Processing (3-Shot) ID: 427fe3968e32005479b9\n",
      "Processing (3-Shot) ID: 06b9ed3f803e3d5796ed\n",
      "Processing (3-Shot) ID: 5090f573b09ac3050824\n",
      "Processing (3-Shot) ID: d912709b7341dd86ba39\n",
      "Results saved successfully to '../StrategyQA/llm_predictions/few_shot/3shot_Gemma-7b-it_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
