{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6300920",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b160bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import re\n",
    "# !pip install -U transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9bd68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac354c",
   "metadata": {},
   "source": [
    "##### $Model$ $Access$\n",
    "\n",
    "In order to access to the model we need to:\n",
    "1. Visit the webpage of the model: https://huggingface.co/meta-llama/Llama-3.2-3B\n",
    "2. Log in to our HF account\n",
    "3. Read and Accept the Use Policies of Meta \n",
    "4. Write yout contact information (email and username)\n",
    "5. Make a request\n",
    "6. An email for validation will be sent\n",
    "7. Run `from huggingface_hub import notebook_login` and `notebook_login()` \n",
    "8. Press the link shown. It will take you to HF and create a new token with `read` rights\n",
    "9. Copy the token and paste it in the notebook and press `Enter`\n",
    "\n",
    " *It might take some minutes*...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb38a6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d768c3e32ca4a1eaf9a604b68b90056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166b1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name and configuration\n",
    "model_name = \"meta-llama/Llama-3.2-3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356db50",
   "metadata": {},
   "source": [
    "##### $Bnb$ $Configuration$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc535d6",
   "metadata": {},
   "source": [
    "\n",
    "The `bnb_config` creates a configuration to shrink the language model. \n",
    "\n",
    "* *Compresses the Model:* It tells the system to load the model in a \"compressed\" 4-bit format instead of its full 16-bit size.\n",
    "* *Saves Memory:* This makes the model about 4 times smaller, allowing it to run on computers with less memory (RAM and VRAM).\n",
    "* *Maintains Performance:* It uses clever tricks (like doing the actual math in 16-bit) to ensure the shrunken model is still fast and accurate.\n",
    "\n",
    "Essentially, it's a set of instructions to make a huge model fit on the computer with minimal loss in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d26bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edf8e1",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f7a08",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer, makes sure there's a padding token (for making all inputs the same length), and if no chat template is found, it manually adds one specifically for Llama 3.x models to format conversations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583e7c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat template not found. Setting LLaMA 3.2 chat template manually.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set chat_template if missing\n",
    "if not hasattr(tokenizer, \"chat_template\") or tokenizer.chat_template is None:\n",
    "    print(\"Chat template not found. Setting LLaMA 3.2 chat template manually.\")\n",
    "    tokenizer.chat_template = (\n",
    "        \"<|begin_of_text|>{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'user' %}\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"{% if add_generation_prompt %}\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"{% endif %}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e3b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093150c3308d4fe0a91932e397aa749e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # or torch.bfloat16 if supported\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9dec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9dec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the quantized model \n",
    "# local_model_path = \"/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/models/my_local_quantized_llama_3b\"\n",
    "\n",
    "# print(f\"Loading quantized model from: {local_model_path}\")\n",
    "\n",
    "# # Load the model and tokenizer directly from the local path.\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     local_model_path,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16 \n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# print(\"Successfully loaded quantized model from local files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0101f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  $Zero$ $Shot$ $vs.$ $Few$ $Shot$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be12753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "base_results_folder = '../HotpotQA/llm_predictions/'\n",
    "zero_shot_folder = os.path.join(base_results_folder, 'zero_shot') # Zero-shot predictions\n",
    "few_shots_folder = os.path.join(base_results_folder, 'few_shot')  # Few-shot predictions\n",
    "os.makedirs(zero_shot_folder, exist_ok=True)\n",
    "os.makedirs(few_shots_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079d4b4",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a0fd4",
   "metadata": {},
   "source": [
    "For each question, it constructs a prompt instructing the model to break down the complex question into smaller, step-by-step sub-questions. It then uses the tokenizer to convert these messages into input IDs, adds an attention mask, and generates a response from the model. Finally, it decodes the model's output and stores the original question and its zero-shot decomposition. The function returns a list of dictionaries containing these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886954f5",
   "metadata": {},
   "source": [
    "* **`input_ids`**: This is the prompt, but translated into a numerical format (a list of numbers) that the model can understand. Each number corresponds to a word or part of a word.\n",
    "\n",
    "* **`attention_mask`**: This is a list of 1s and 0s that has the same length as the input_ids. It tells the model which tokens are real words to pay attention to (a 1) and which ones are just padding that should be ignored (a 0). \n",
    "\n",
    "* **`pad_token_id`**: This specifies the ID of the special token that is used to \"fill up\" shorter sentences when we process multiple sentences at once (batching). It's directly related to the attention_mask.\n",
    "\n",
    "* **`max_new_tokens`**: This sets the maximum length of the generated response. max_new_tokens=512 tells the model, \"Do not write more than 512 new tokens after the prompt.\" This prevents it from writing forever.\n",
    "\n",
    "* **`early_stopping`**: If set to False, the model might finish its thought in 80 words but feel compelled to keep adding filler words to get closer to the 300-word limit. With early stopping once it writes its thought is complete, it just stops.\n",
    "\n",
    "\n",
    "* **`do_sample`**:\n",
    "\t* **do_sample=False**: This forces the model to be deterministic. Every time it generates a new word, it chooses the single word that it calculates as being the most statistically likely to come next. When we compare the models, we want to compare their \"best, most probable\" attempt at the problem.\n",
    "\t* **do_sample=True**: This tells the model to be creative and less predictable. Instead of always picking the #1 most likely word, it might pick the #2 or #3 most likely word, based on a random sample (controlled by parameters like temperature).\n",
    "\n",
    "Since the task is analytical (decomposing a problem) and not creative, the deterministic approach is better. We choose do_sample=False for all the baseline experiments to ensure your results are stable and repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91827aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot_experiment(model, tokenizer, questions):\n",
    "    results = []\n",
    "    for item in questions:\n",
    "        print(f\"Processing (Zero-Shot) ID: {item['id']}\")\n",
    "        \n",
    "\n",
    "\t\t# You are a helpful assistant that breaks down complex questions into smaller steps.\\n\n",
    "        #   Decompose the following question into 5â€“7 reasoning steps needed to solve it.\n",
    "        #   Each step should be a simple sub-question that contributes to solving the overall problem.\\n\\n\n",
    "\n",
    "\n",
    "\n",
    "\t\t  #You are a helpful assistant that breaks down complex questions into smaller steps.\\n\n",
    "            #Your task is to break the following complex question into simpler, more manageable sub-questions\n",
    "            #that contribute to solving the overall problem.\\n\n",
    "          # \"\"\"You are a helpful assistant that breaks down complex questions into smaller steps.\\n\n",
    "          # Decompose the following question into a few reasoning steps that contribute to solving the overall problem.\\n\\n\n",
    "\n",
    "        prompt = (\n",
    "            f\"\"\"\n",
    "            You are a helpful assistant.Your task is to break down the following question\n",
    "             into a few smaller questions that contribute to solving the overall problem.\\n\\n\n",
    "\n",
    "        \n",
    "\n",
    "          Complex Question: {item['question']}\\n\\n\n",
    "\n",
    "          Step-by-Step plan:\"\"\"\n",
    "        )\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Add attention mask\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Explicitly set pad_token_id\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,  # Optional: Adjust to reduce repetition\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        decomposition_result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        # Store the result \n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item['decomposition'],\n",
    "            f\"zero_shot_decomposition\": decomposition_result\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986502c",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d331baf",
   "metadata": {},
   "source": [
    "This function, performs few-shots learning.\n",
    "\n",
    "It takes a list of questions, a list of high-quality example question/decomposition pairs (`shot_examples`), and the number of examples (`num_shots`) to use.\n",
    "\n",
    "The function first uses a prompt that includes the specified number of examples, showing the model how to decompose complex questions into simpler sub-questions. Then, for each question, it sends the prompt to the language model, generates a decomposition, and stores the results. This approach helps the model understand the desired output format and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc4f1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot_experiment(model, tokenizer, data, shot_examples, num_shots=3):\n",
    "    \n",
    "    if num_shots > len(shot_examples):\n",
    "        raise ValueError(f\"You asked for {num_shots} shots, but only {len(shot_examples)} are available.\")\n",
    "\n",
    "    results = []\n",
    "    for item in data:\n",
    "        print(f\"Processing ({num_shots}-Shot) ID: {item['id']}\")\n",
    "\n",
    "        # Build the Conversation List (the 'messages' variable) \n",
    "        \n",
    "        # Start with the system prompt that defines the AI's role and overall task\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert assistant. Your task is to break down the following question into a few smaller questions that contribute to solving the overall problem. Give only the decomposition steps, not the final answer.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Add the few-shot examples as a series of user/assistant turns\n",
    "        for example in shot_examples[:num_shots]:\n",
    "            messages.append({\"role\": \"user\", \"content\": example['question']})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": example['decomposition']})\n",
    "\n",
    "        # Finally, add the actual user question we want the model to answer now\n",
    "        messages.append({\"role\": \"user\", \"content\": item['question']})\n",
    "\n",
    "        # Generate the Response\n",
    "        \n",
    "        # The input to apply_chat_template is now the structured 'messages' list\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Add attention mask\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Explicitly set pad_token_id\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,  # Optional: Adjust to reduce repetitio\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        generated_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        decomposition_result = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        # Store the result \n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item['decomposition'],\n",
    "            f\"{num_shots}_shot_decomposition\": decomposition_result\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c2b75",
   "metadata": {},
   "source": [
    "##### $Save$ $the$ $results$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cefccb",
   "metadata": {},
   "source": [
    "*Once the model processes the questions, its predictions for the zero-shot and few-shot experiments are saved in the corresponding folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f092d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, folder, filename):\n",
    "\n",
    "    # Make sure the folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Construct the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "\n",
    "    # Save the file\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122b0e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb58b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "hotpot_dataset_path = \"../HotpotQA/HotpotQA_dataset/hotpot_dataset.json\" # Define the path to the hotpot dataset\n",
    "\n",
    "# Load the dataset\n",
    "with open(hotpot_dataset_path, \"r\") as file:\n",
    "    hotpot_data = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5ac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question details:\n",
      "ID: 5a8b57f25542995d1e6f1371\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Answer: yes\n",
      "Supporting sentences: ['Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.', 'Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.', 'Aggregating the above we conclude that the answer is: yes']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Decomposition: ['What nationality Scott Derrickson had?', 'What nationality Ed Wood had?', 'Was the nationality the same?']\n"
     ]
    }
   ],
   "source": [
    "# Display the first question\n",
    "print(\"First question details:\")\n",
    "print(\"ID:\", hotpot_data[0][\"id\"])\n",
    "print(\"Question:\", hotpot_data[0][\"question\"])\n",
    "print(\"Answer:\", hotpot_data[0][\"answer\"])\n",
    "print(\"Supporting sentences:\", hotpot_data[0][\"supporting_sentences\"])\n",
    "print(\"-\"*150)\n",
    "print(\"Decomposition:\", hotpot_data[0][\"decomposition\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d080b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"Llama-3.2-3B_Hotpot_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b884543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "few_shot_hotpot_examples_path = \"../HotpotQA/HotpotQA_dataset/hotpot_few_shot.json\"\n",
    "with open(few_shot_hotpot_examples_path, \"r\") as file:\n",
    "    shot_examples = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bab15",
   "metadata": {},
   "source": [
    "##### $Experiments$ $Run$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39e39767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (Zero-Shot) ID: 5a8b57f25542995d1e6f1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (Zero-Shot) ID: 5a8c7595554299585d9e36b6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (Zero-Shot) ID: 5a85ea095542994775f606a8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (Zero-Shot) ID: 5adbf0a255429947ff17385a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (Zero-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "zero_shot_results = run_zero_shot_experiment(model, tokenizer, hotpot_data)\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88d6ed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: 5a8b57f25542995d1e6f1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (3-Shot) ID: 5a8c7595554299585d9e36b6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (3-Shot) ID: 5a85ea095542994775f606a8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (3-Shot) ID: 5adbf0a255429947ff17385a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (3-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "three_shot_results = run_few_shot_experiment(model, tokenizer, hotpot_data, shot_examples, num_shots=3)\n",
    "save_results_to_json(three_shot_results, few_shots_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
