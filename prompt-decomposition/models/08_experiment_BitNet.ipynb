{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531f1f13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35ae351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import re\n",
    "# !pip install -U transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f190f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d26876",
   "metadata": {},
   "source": [
    "REQUIREMENTS: pip install git+https://github.com/huggingface/transformers.git@096f25ae1f501a084d8ff2dcaf25fbc2bd60eba4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e148e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name \n",
    "model_name = 'microsoft/bitnet-b1.58-2B-4T'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9c5f4",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38c1f7",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35bd1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for microsoft/bitnet-b1.58-2B-4T loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer. The library will handle the chat template automatically.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "print(f\"Tokenizer for {model_name} loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c6153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitNet model (loaded as Llama) and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"BitNet model  and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "412f03c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "microsoft/bitnet-b1.58-2B-4T does not appear to have a file named configuration_bitnet.py. Checkout 'https://huggingface.co/microsoft/bitnet-b1.58-2B-4T/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmicrosoft/bitnet-b1.58-2B-4T\u001b[39m\u001b[33m\"\u001b[39m, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/bitnet-b1.58-2B-4T\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:547\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    545\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1211\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1206\u001b[39m     trust_remote_code = resolve_trust_remote_code(\n\u001b[32m   1207\u001b[39m         trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n\u001b[32m   1208\u001b[39m     )\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m     config_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m     config_class.register_for_auto_class()\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:570\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m     code_revision = revision\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m final_module = \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload=force_download)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:372\u001b[39m, in \u001b[36mget_cached_module_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m new_files = []\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     resolved_module_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local \u001b[38;5;129;01mand\u001b[39;00m cached_module != resolved_module_file:\n\u001b[32m    386\u001b[39m         new_files.append(module_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:312\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    255\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    256\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    257\u001b[39m     **kwargs,\n\u001b[32m    258\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:573\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     revision_ = \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m revision\n\u001b[32m    570\u001b[39m     msg = (\n\u001b[32m    571\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_entries[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_entries) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfiles named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(*missing_entries,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    572\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Checkout \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for available files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     )\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# Remove potential missing entries (we can silently remove them at this point based on the flags)\u001b[39;00m\n\u001b[32m    579\u001b[39m resolved_files = [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[31mOSError\u001b[39m: microsoft/bitnet-b1.58-2B-4T does not appear to have a file named configuration_bitnet.py. Checkout 'https://huggingface.co/microsoft/bitnet-b1.58-2B-4T/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/bitnet-b1.58-2B-4T\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/bitnet-b1.58-2B-4T\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a536cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  $Zero$ $Shots$ $vs.$ $Few$ $Shots$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "base_results_folder = '/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/llm_predictions/'\n",
    "zero_shot_folder = os.path.join(base_results_folder, 'zero_shot') # Zero-shot predictions\n",
    "few_shots_folder = os.path.join(base_results_folder, 'few_shot')  # Few-shot predictions\n",
    "os.makedirs(zero_shot_folder, exist_ok=True)\n",
    "os.makedirs(few_shots_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a893701",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa50631",
   "metadata": {},
   "source": [
    "For each question, it constructs a prompt instructing the model to break down the complex question into smaller, step-by-step sub-questions. It then uses the tokenizer to convert these messages into input IDs, adds an attention mask, and generates a response from the model. Finally, it decodes the model's output and stores the original question and its zero-shot decomposition. The function returns a list of dictionaries containing these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f09dbb",
   "metadata": {},
   "source": [
    "* **`input_ids`**: This is the prompt, but translated into a numerical format (a list of numbers) that the model can understand. Each number corresponds to a word or part of a word.\n",
    "\n",
    "* **`attention_mask`**: This is a list of 1s and 0s that has the same length as the input_ids. It tells the model which tokens are real words to pay attention to (a 1) and which ones are just padding that should be ignored (a 0). \n",
    "\n",
    "* **`pad_token_id`**: This specifies the ID of the special token that is used to \"fill up\" shorter sentences when we process multiple sentences at once (batching). It's directly related to the attention_mask.\n",
    "\n",
    "* **`max_new_tokens`**: This sets the maximum length of the generated response. max_new_tokens=512 tells the model, \"Do not write more than 512 new tokens after the prompt.\" This prevents it from writing forever.\n",
    "\n",
    "* **`early_stopping`**: If set to False, the model might finish its thought in 80 words but feel compelled to keep adding filler words to get closer to the 300-word limit. With early stopping once it writes its thought is complete, it just stops.\n",
    "\n",
    "\n",
    "* **`do_sample`**:\n",
    "\t* **do_sample=False**: This forces the model to be deterministic. Every time it generates a new word, it chooses the single word that it calculates as being the most statistically likely to come next. When we compare the models, we want to compare their \"best, most probable\" attempt at the problem.\n",
    "\t* **do_sample=True**: This tells the model to be creative and less predictable. Instead of always picking the #1 most likely word, it might pick the #2 or #3 most likely word, based on a random sample (controlled by parameters like temperature).\n",
    "\n",
    "Since the task is analytical (decomposing a problem) and not creative, the deterministic approach is better. We choose do_sample=False for all the baseline experiments to ensure your results are stable and repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f456488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot_experiment(model, tokenizer, questions):\n",
    "    results = []\n",
    "\n",
    "    for item in questions:\n",
    "        print(f\"Processing (Zero-Shot) ID: {item['id']}\")\n",
    "\n",
    "        # Construct the raw string prompt\n",
    "        prompt_text = (\n",
    "            \"You are a helpful assistant. Your task is to break down the following question \"\n",
    "            \"into a few smaller questions that contribute to solving the overall problem.\\n\\n\"\n",
    "            f\"Complex Question: {item['question']}\\n\\n\"\n",
    "            \"Step-by-Step plan:\"\n",
    "        )\n",
    "\n",
    "        # Apply chat template if needed (LLaDA-Instruct models require it)\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "\n",
    "        # Tokenize into input IDs\n",
    "        input_ids = tokenizer(formatted_prompt)['input_ids']\n",
    "        input_ids = torch.tensor(input_ids).to(model.device).unsqueeze(0)  # Shape: (1, L)\n",
    "\n",
    "        # Run LLaDA's generation function\n",
    "        output_ids = generate(\n",
    "            model=model,\n",
    "            prompt=input_ids,\n",
    "            steps=128,\n",
    "            gen_length=128,\n",
    "            block_length=32,\n",
    "            temperature=0.0,\n",
    "            cfg_scale=0.0,\n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "\n",
    "        # Decode the generated output after the prompt\n",
    "        generated_text = tokenizer.batch_decode(output_ids[:, input_ids.shape[1]:], skip_special_tokens=True)[0].strip()\n",
    "\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item.get('decomposition', ''),  # fallback in case not provided\n",
    "            \"zero_shot_decomposition\": generated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e7211",
   "metadata": {},
   "source": [
    "##### $Few$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667993a8",
   "metadata": {},
   "source": [
    "This function, performs few-shots learning.\n",
    "\n",
    "It takes a list of questions, a list of high-quality example question/decomposition pairs (`shot_examples`), and the number of examples (`num_shots`) to use.\n",
    "\n",
    "The function first uses a prompt that includes the specified number of examples, showing the model how to decompose complex questions into simpler sub-questions. Then, for each question, it sends the prompt to the language model, generates a decomposition, and stores the results. This approach helps the model understand the desired output format and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate  # LLaDA's custom generate function\n",
    "\n",
    "def run_few_shot_experiment(model, tokenizer, data, shot_examples, num_shots=3):\n",
    "    if num_shots > len(shot_examples):\n",
    "        raise ValueError(f\"You asked for {num_shots} shots, but only {len(shot_examples)} are available.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for item in data:\n",
    "        print(f\"Processing ({num_shots}-Shot) ID: {item['id']}\")\n",
    "\n",
    "        # Build the chat-style prompt using few-shot examples\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert assistant. Your task is to break down the following question into a few smaller questions that contribute to solving the overall problem. Give only the decomposition steps, not the final answer.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Add few-shot examples\n",
    "        for example in shot_examples[:num_shots]:\n",
    "            decomposition_text = example['decomposition']\n",
    "            if isinstance(decomposition_text, list):\n",
    "                decomposition_text = \"\\n\".join(decomposition_text)\n",
    "            messages.append({\"role\": \"user\", \"content\": example['question']})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": decomposition_text})\n",
    "\n",
    "        # Add the actual question to be decomposed\n",
    "        messages.append({\"role\": \"user\", \"content\": item['question']})\n",
    "\n",
    "        # Convert chat messages into a prompt string (not tokenized yet)\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "\n",
    "        # Tokenize to tensor (batch of size 1)\n",
    "        input_ids = tokenizer(prompt_text)['input_ids']\n",
    "        input_ids = torch.tensor(input_ids).to(model.device).unsqueeze(0)\n",
    "\n",
    "        # Generate output using LLaDA\n",
    "        output_ids = generate(\n",
    "            model=model,\n",
    "            prompt=input_ids,\n",
    "            steps=128,\n",
    "            gen_length=128,\n",
    "            block_length=32,\n",
    "            temperature=0.0,\n",
    "            cfg_scale=0.0,\n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "\n",
    "        # Decode only the generated portion\n",
    "        generated_text = tokenizer.batch_decode(\n",
    "            output_ids[:, input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "\n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item.get('decomposition', ''),\n",
    "            f\"{num_shots}_shot_decomposition\": generated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac611512",
   "metadata": {},
   "source": [
    "##### $Save$ $the$ $results$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73397d61",
   "metadata": {},
   "source": [
    "*Once the model processes the questions, its predictions for the zero-shot and few-shot experiments are saved in the corresponding folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, folder, filename):\n",
    "\n",
    "    # Make sure the folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Construct the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "\n",
    "    # Save the file\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ccc06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c9159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "hotpot_dataset_path = \"/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/HotpotQA_dataset/hotpot_dataset.json\" # Define the path to the hotpot dataset\n",
    "\n",
    "# Load the dataset\n",
    "with open(hotpot_dataset_path, \"r\") as file:\n",
    "    hotpot_data = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493643d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question details:\n",
      "ID: 5a8b57f25542995d1e6f1371\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Answer: yes\n",
      "Supporting sentences: ['Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.', 'Edward Davis Wood Jr. (October 10, 1924 – December 10, 1978) was an American filmmaker, actor, writer, producer, and director.', 'Aggregating the above we conclude that the answer is: yes']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Decomposition: ['What nationality Scott Derrickson had?', 'What nationality Ed Wood had?', 'Was the nationality the same?']\n"
     ]
    }
   ],
   "source": [
    "# Display the first question\n",
    "print(\"First question details:\")\n",
    "print(\"ID:\", hotpot_data[0][\"id\"])\n",
    "print(\"Question:\", hotpot_data[0][\"question\"])\n",
    "print(\"Answer:\", hotpot_data[0][\"answer\"])\n",
    "print(\"Supporting sentences:\", hotpot_data[0][\"supporting_sentences\"])\n",
    "print(\"-\"*150)\n",
    "print(\"Decomposition:\", hotpot_data[0][\"decomposition\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33898180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"LLaDA-8B-Base_Hotpot_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "few_shot_hotpot_examples_path = \"/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/HotpotQA_dataset/hotpot_few_shot.json\"\n",
    "with open(few_shot_hotpot_examples_path, \"r\") as file:\n",
    "    shot_examples = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a3cc",
   "metadata": {},
   "source": [
    "##### $Experiments$ $Run$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (Zero-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (Zero-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (Zero-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (Zero-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (Zero-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "zero_shot_results = run_zero_shot_experiment(model, tokenizer, hotpot_data)\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec34b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file_name = \"LLaDA-8B-Base_Hotpot_results.json\"  # ✅ no leading slash\n",
    "\n",
    "# save_results_to_json(\n",
    "#     zero_shot_results,\n",
    "#     '/home/lathanasopoulou/capstone/LLaDA',\n",
    "#     model_file_name\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (3-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (3-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (3-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (3-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "three_shot_results = run_few_shot_experiment(model, tokenizer, hotpot_data, shot_examples, num_shots=3)\n",
    "save_results_to_json(three_shot_results, few_shots_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
